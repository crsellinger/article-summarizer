{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: **Caleb Sellinger**\n",
    "\n",
    "### GitHub Repo [HERE](https://github.com/crsellinger/article-summarizer)\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "annotated-types         0.7.0\n",
      "asttokens               3.0.0\n",
      "attrs                   25.3.0\n",
      "beautifulsoup4          4.13.4\n",
      "blis                    1.3.0\n",
      "catalogue               2.0.10\n",
      "certifi                 2025.8.3\n",
      "cffi                    1.17.1\n",
      "charset-normalizer      3.4.2\n",
      "click                   8.2.1\n",
      "cloudpathlib            0.21.1\n",
      "colorama                0.4.6\n",
      "comm                    0.2.3\n",
      "confection              0.1.5\n",
      "contourpy               1.3.3\n",
      "cycler                  0.12.1\n",
      "cymem                   2.0.11\n",
      "debugpy                 1.8.15\n",
      "decorator               5.2.1\n",
      "en_core_web_sm          3.8.0\n",
      "executing               2.2.0\n",
      "fonttools               4.59.0\n",
      "h11                     0.16.0\n",
      "idna                    3.10\n",
      "ipykernel               6.30.1\n",
      "ipython                 9.4.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.8.1\n",
      "kiwisolver              1.4.8\n",
      "langcodes               3.5.0\n",
      "language_data           1.3.0\n",
      "marisa-trie             1.2.1\n",
      "markdown-it-py          3.0.0\n",
      "MarkupSafe              3.0.2\n",
      "matplotlib              3.10.5\n",
      "matplotlib-inline       0.1.7\n",
      "mdurl                   0.1.2\n",
      "murmurhash              1.0.13\n",
      "nest-asyncio            1.6.0\n",
      "numpy                   2.3.2\n",
      "outcome                 1.3.0.post0\n",
      "packaging               25.0\n",
      "parso                   0.8.4\n",
      "pillow                  11.3.0\n",
      "pip                     25.2\n",
      "platformdirs            4.3.8\n",
      "preshed                 3.0.10\n",
      "prompt_toolkit          3.0.51\n",
      "psutil                  7.0.0\n",
      "pure_eval               0.2.3\n",
      "pycparser               2.22\n",
      "pydantic                2.11.7\n",
      "pydantic_core           2.33.2\n",
      "Pygments                2.19.2\n",
      "pyparsing               3.2.3\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pywin32                 311\n",
      "pyzmq                   27.0.1\n",
      "requests                2.32.4\n",
      "rich                    14.1.0\n",
      "selenium                4.34.2\n",
      "setuptools              80.9.0\n",
      "shellingham             1.5.4\n",
      "six                     1.17.0\n",
      "smart_open              7.3.0.post1\n",
      "sniffio                 1.3.1\n",
      "sortedcontainers        2.4.0\n",
      "soupsieve               2.7\n",
      "spacy                   3.8.7\n",
      "spacy-legacy            3.0.12\n",
      "spacy-loggers           1.0.5\n",
      "srsly                   2.5.1\n",
      "stack-data              0.6.3\n",
      "thinc                   8.3.6\n",
      "tornado                 6.5.1\n",
      "tqdm                    4.67.1\n",
      "traitlets               5.14.3\n",
      "trio                    0.30.0\n",
      "trio-websocket          0.12.2\n",
      "typer                   0.16.0\n",
      "typing_extensions       4.14.1\n",
      "typing-inspection       0.4.1\n",
      "urllib3                 2.5.0\n",
      "wasabi                  1.1.3\n",
      "wcwidth                 0.2.13\n",
      "weasel                  0.4.1\n",
      "websocket-client        1.8.0\n",
      "wheel                   0.45.1\n",
      "wrapt                   1.17.2\n",
      "wsproto                 1.2.0\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import selenium\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they still aren covering0hold your fucking stocks0party on dudes1something shady going on04 00am pre market hype0diamond hands relax as my 3d printer sings you the song of it people army1dogeeeee0rkt dividends but not because of the dividends0i used all my stimmy0zom looks good to go1so basically am moonkey0mertgag ageddon uwmc finally the play1doing my part to show the market how meaningless robinhood is-1aal american airlines year long consolidation breakout cheers to the moom and beyond1proud of my holdings m down nothing comparing to big boys but if ever single one in the page sell ll...-1january 28 2021 7 14am pst never forget1webull0even htmw won let me trade and it fake money1put all in on blackberry dip0market level 2 data0uk brothers and sisters freetrade are still allowing gme bb amc trading and instant account topups o...0counterfeit shares and what we should be doing right now1public won let buys on the dip1unite against the machine0you ve won there are no evil doers left on the other end of the trade-1forced yolo 52k into gamestop like the stock11 800 flowers dd0palantir pltr is low bottom company and has become too important to fail here is why-1unpinned daily discussion thread for february 09 20210did you just seen this shit-1etoro is allowing to buy gme still0anyone knows why slv jumped up suddenly0imgn through the roof or underground0when this rocket0bb 60 down0nmrk yolo update 100k 188k1aal herd immunity by april 15k in march 19th calls0how do you buy stocks0a little late but thought this would be appreciated 36 into 5000 in two days1denzel washington once said if you don read the newspaper you re uninformed if you do read the newsp...-1found nice bottom melt up begins buy the dip1amc the moon got lil closer0everyone0yesterday if you followed my advice on tlry you be up 100 in 24 hrs today let talk about wkhs osk xp...0i holding strong with you retards together strong1i explain things to idiots part 2 puts and couple clarifications about calls0portfolio tanked today but bought the dip and m still holding on0hamilton meets gamestop our rallying cry r wallstreetbets exclusive proudly made by ptgauth and in t...1no matter how rough it gets dont sell hold and hold some more we will prevail-1buy nok0\n"
     ]
    }
   ],
   "source": [
    "# Webpage uses JavaScript to load content, need Selenium to run JS to pull content\n",
    "# BS4 does not run scripts and was only pulling root div with scripts\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "url = \"https://www.kaggle.com/datasets/sudan007kaggler/reddit-rwallstreet-bets-posts-dataset-labelled/data\"\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the page\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Wait for the specific div with role=\"rowgroup\" to be present\n",
    "    # Wait a max of 20 seconds.\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[role=\"rowgroup\"]')))\n",
    "    \n",
    "    # Get the page source after JavaScript has loaded\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Now, parse the fully loaded HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find table\n",
    "    element = soup.find('div', attrs={'role': 'rowgroup'})\n",
    "\n",
    "    if element:\n",
    "        with open('content.pkl','wb') as file:\n",
    "            pickle.dump(element.text, file)\n",
    "        # print(element.prettify)\n",
    "        with open('content.pkl','rb') as file:\n",
    "            content = pickle.load(file)\n",
    "        print(content)\n",
    "    else:\n",
    "        print(\"Could not find the div with role='rowgroup' even after waiting.\")\n",
    "\n",
    "finally:\n",
    "    # Close browser window\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div role=\"rowgroup\"><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>they still aren covering</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>hold your fucking stocks</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>party on dudes</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>something shady going on</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>4 00am pre market hype</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>diamond hands relax as my 3d printer sings you the song of it people army</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>dogeeeee</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>rkt dividends but not because of the dividends</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>i used all my stimmy</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>zom looks good to go</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>so basically am moonkey</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>mertgag ageddon uwmc finally the play</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>doing my part to show the market how meaningless robinhood is</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>aal american airlines year long consolidation breakout cheers to the moom and beyond</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>proud of my holdings m down nothing comparing to big boys but if ever single one in the page sell ll...</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>january 28 2021 7 14am pst never forget</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>webull</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>even htmw won let me trade and it fake money</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>put all in on blackberry dip</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>market level 2 data</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>uk brothers and sisters freetrade are still allowing gme bb amc trading and instant account topups o...</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>counterfeit shares and what we should be doing right now</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>public won let buys on the dip</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>unite against the machine</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>you ve won there are no evil doers left on the other end of the trade</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>forced yolo 52k into gamestop like the stock</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>1 800 flowers dd</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>palantir pltr is low bottom company and has become too important to fail here is why</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>unpinned daily discussion thread for february 09 2021</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>did you just seen this shit</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>etoro is allowing to buy gme still</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>anyone knows why slv jumped up suddenly</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>imgn through the roof or underground</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>when this rocket</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>bb 60 down</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>nmrk yolo update 100k 188k</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>aal herd immunity by april 15k in march 19th calls</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>how do you buy stocks</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>a little late but thought this would be appreciated 36 into 5000 in two days</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>denzel washington once said if you don read the newspaper you re uninformed if you do read the newsp...</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>found nice bottom melt up begins buy the dip</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>amc the moon got lil closer</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>everyone</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>yesterday if you followed my advice on tlry you be up 100 in 24 hrs today let talk about wkhs osk xp...</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>i holding strong with you retards together strong</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>i explain things to idiots part 2 puts and couple clarifications about calls</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>portfolio tanked today but bought the dip and m still holding on</td><td>0</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>hamilton meets gamestop our rallying cry r wallstreetbets exclusive proudly made by ptgauth and in t...</td><td>1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>no matter how rough it gets dont sell hold and hold some more we will prevail</td><td>-1</td></tr></span><span class=\"sc-iMGFoU iLsOEd\"><tr class=\"sc-ilnnPk sc-hfsVOy dSkrLE hnoKRX\"><td>buy nok</td><td>0</td></tr></span></div>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
