{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: **Caleb Sellinger**\n",
    "\n",
    "### GitHub Repo [HERE](https://github.com/crsellinger/article-summarizer)\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- -----------\n",
      "annotated-types         0.7.0\n",
      "asttokens               3.0.0\n",
      "attrs                   25.3.0\n",
      "beautifulsoup4          4.13.4\n",
      "blis                    1.3.0\n",
      "catalogue               2.0.10\n",
      "certifi                 2025.8.3\n",
      "cffi                    1.17.1\n",
      "charset-normalizer      3.4.2\n",
      "click                   8.2.1\n",
      "cloudpathlib            0.21.1\n",
      "colorama                0.4.6\n",
      "comm                    0.2.3\n",
      "confection              0.1.5\n",
      "contourpy               1.3.3\n",
      "cycler                  0.12.1\n",
      "cymem                   2.0.11\n",
      "debugpy                 1.8.15\n",
      "decorator               5.2.1\n",
      "en_core_web_sm          3.8.0\n",
      "executing               2.2.0\n",
      "fonttools               4.59.0\n",
      "h11                     0.16.0\n",
      "idna                    3.10\n",
      "ipykernel               6.30.1\n",
      "ipython                 9.4.0\n",
      "ipython_pygments_lexers 1.1.1\n",
      "jedi                    0.19.2\n",
      "Jinja2                  3.1.6\n",
      "joblib                  1.5.1\n",
      "jupyter_client          8.6.3\n",
      "jupyter_core            5.8.1\n",
      "kiwisolver              1.4.8\n",
      "langcodes               3.5.0\n",
      "language_data           1.3.0\n",
      "marisa-trie             1.2.1\n",
      "markdown-it-py          3.0.0\n",
      "MarkupSafe              3.0.2\n",
      "matplotlib              3.10.5\n",
      "matplotlib-inline       0.1.7\n",
      "mdurl                   0.1.2\n",
      "murmurhash              1.0.13\n",
      "nest-asyncio            1.6.0\n",
      "nltk                    3.9.1\n",
      "numpy                   2.3.2\n",
      "outcome                 1.3.0.post0\n",
      "packaging               25.0\n",
      "parso                   0.8.4\n",
      "pillow                  11.3.0\n",
      "pip                     25.2\n",
      "platformdirs            4.3.8\n",
      "preshed                 3.0.10\n",
      "prompt_toolkit          3.0.51\n",
      "psutil                  7.0.0\n",
      "pure_eval               0.2.3\n",
      "pycparser               2.22\n",
      "pydantic                2.11.7\n",
      "pydantic_core           2.33.2\n",
      "Pygments                2.19.2\n",
      "pyparsing               3.2.3\n",
      "PySocks                 1.7.1\n",
      "python-dateutil         2.9.0.post0\n",
      "pywin32                 311\n",
      "pyzmq                   27.0.1\n",
      "regex                   2025.7.34\n",
      "requests                2.32.4\n",
      "rich                    14.1.0\n",
      "selenium                4.34.2\n",
      "setuptools              80.9.0\n",
      "shellingham             1.5.4\n",
      "six                     1.17.0\n",
      "smart_open              7.3.0.post1\n",
      "sniffio                 1.3.1\n",
      "sortedcontainers        2.4.0\n",
      "soupsieve               2.7\n",
      "spacy                   3.8.7\n",
      "spacy-legacy            3.0.12\n",
      "spacy-loggers           1.0.5\n",
      "spacytextblob           5.0.0\n",
      "srsly                   2.5.1\n",
      "stack-data              0.6.3\n",
      "textblob                0.19.0\n",
      "thinc                   8.3.6\n",
      "tornado                 6.5.1\n",
      "tqdm                    4.67.1\n",
      "traitlets               5.14.3\n",
      "trio                    0.30.0\n",
      "trio-websocket          0.12.2\n",
      "typer                   0.16.0\n",
      "typing_extensions       4.14.1\n",
      "typing-inspection       0.4.1\n",
      "urllib3                 2.5.0\n",
      "wasabi                  1.1.3\n",
      "wcwidth                 0.2.13\n",
      "weasel                  0.4.1\n",
      "websocket-client        1.8.0\n",
      "wheel                   0.45.1\n",
      "wrapt                   1.17.2\n",
      "wsproto                 1.2.0\n",
      "All prereqs installed.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import selenium\n",
    "\n",
    "!pip list\n",
    "\n",
    "print('All prereqs installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webpage uses JavaScript to load content, need Selenium to run JS to pull content\n",
    "# BS4 does not run scripts and was only pulling root div with scripts\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "url = \"https://www.kaggle.com/datasets/sudan007kaggler/reddit-rwallstreet-bets-posts-dataset-labelled/data\"\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the page\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # Wait for the specific div with role=\"rowgroup\" to be present\n",
    "    # Wait a max of 20 seconds.\n",
    "    # This only pulls what is originally loaded onto the page by JS. From testing, its around 80 rows.\n",
    "    # In order to get all 41,981, I would need to scroll down the table and wait for it to load. I don't know if scripting a scroll action is possible with Selenium\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[role=\"rowgroup\"]')))\n",
    "\n",
    "    # Get the page source after JavaScript has loaded\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Now, parse the fully loaded HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find table\n",
    "    element = soup.find_all('td')\n",
    "    # print(element[8])\n",
    "\n",
    "    if element:\n",
    "        with open('content.pkl','wb') as file:\n",
    "            # Strip text before pickling, was running into max recursion depth error bc html was too complicated\n",
    "            # Using join() method to combine all strings in list into one string for Doc element\n",
    "            pickle.dump(\"\\n\".join([e.get_text(strip=True) for e in element]), file)\n",
    "        # print(element.prettify)\n",
    "    else:\n",
    "        print(\"Could not find the div with role='rowgroup' even after waiting.\")\n",
    "\n",
    "finally:\n",
    "    # Close browser window\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 76\n",
      "Post 0: 0.0\n",
      "Post 1: -0.6\n",
      "Post 2: 0.0\n",
      "Post 3: -0.25\n",
      "Post 4: 0.0\n",
      "Post 5: 0.0\n",
      "Post 6: 0.0\n",
      "Post 7: 0.0\n",
      "Post 8: 0.0\n",
      "Post 9: 0.7\n",
      "Post 10: 0.0\n",
      "Post 11: 0.0\n",
      "Post 12: -0.5\n",
      "Post 13: -0.025\n",
      "Post 14: 0.14325396825396827\n",
      "Post 15: 0.0\n",
      "Post 16: 0.0\n",
      "Post 17: -0.5\n",
      "Post 18: 0.0\n",
      "Post 19: 0.0\n",
      "Post 20: 0.0\n",
      "Post 21: 0.2857142857142857\n",
      "Post 22: 0.0\n",
      "Post 23: 0.0\n",
      "Post 24: 0.125\n",
      "Post 25: -0.30000000000000004\n",
      "Post 26: 0.0\n",
      "Post 27: -0.033333333333333326\n",
      "Post 28: 0.0\n",
      "Post 29: -0.2\n",
      "Post 30: 0.0\n",
      "Post 31: 0.0\n",
      "Post 32: 0.0\n",
      "Post 33: 0.0\n",
      "Post 34: -0.15555555555555556\n",
      "Post 35: 0.0\n",
      "Post 36: 0.0\n",
      "Post 37: 0.0\n",
      "Post 38: -0.09583333333333333\n",
      "Post 39: 0.0\n",
      "Post 40: 0.6\n",
      "Post 41: 0.0\n",
      "Post 42: 0.0\n",
      "Post 43: 0.0\n",
      "Post 44: -0.011111111111111108\n",
      "Post 45: -0.8\n",
      "Post 46: 0.0\n",
      "Post 47: 0.8\n",
      "Post 48: 0.2\n",
      "Post 49: 0.0\n",
      "Post 50: 0.0\n",
      "Post 51: 0.0\n",
      "Post 52: 0.0\n",
      "Post 53: 0.0\n",
      "Post 54: 0.0\n",
      "Post 55: 0.0\n",
      "Post 56: 0.0\n",
      "Post 57: 0.0\n",
      "Post 58: 0.0\n",
      "Post 59: 0.0\n",
      "Post 60: 0.0\n",
      "Post 61: 0.0\n",
      "Post 62: 0.0\n",
      "Post 63: 0.0\n",
      "Post 64: 0.0\n",
      "Post 65: 0.0\n",
      "Post 66: 0.0\n",
      "Post 67: 0.0\n",
      "Post 68: 0.0\n",
      "Post 69: 0.0\n",
      "Post 70: 0.0\n",
      "Post 71: 0.0\n",
      "Post 72: 0.0\n",
      "Post 73: 0.0\n",
      "Post 74: 0.0\n",
      "Post 75: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Spacy Pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacytextblob\")\n",
    "\n",
    "# Reading file into python object\n",
    "with open('content.pkl','rb') as file:\n",
    "    content = pickle.load(file)\n",
    "\n",
    "# Splitting every other line in file, transforming to Doc type, and appending into list\n",
    "# Need Doc type for polarity score\n",
    "list_sentences = content.splitlines()[8::2]\n",
    "list_sentences = [nlp(line) for line in list_sentences]\n",
    "# print(list_sentences)\n",
    "\n",
    "# Print number of sentences\n",
    "print(f\"Number of posts: {len(list_sentences)}\")\n",
    "\n",
    "# Polarity scores for each line\n",
    "list_of_scores = [line._.blob.polarity for line in list_sentences]\n",
    "for i, s in enumerate(list_of_scores):\n",
    "    print(f\"Post {i}: {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common tokens:\n",
      "jul: 20\n",
      "aug: 6\n",
      "dip: 4\n",
      "buy: 4\n",
      "hold: 3\n",
      "values | aren | covering | hold | fucking | stocks | party | dudes | shady | going | pre | market | hype | diamond | hands | relax | printer | sings | song | people | army | dogeeeee | rkt | dividends | dividends | stimmy | zom | looks | good | basically | moonkey | mertgag | ageddon | uwmc | finally | play | market | meaningless | robinhood | aal | american | airlines | year | long | consolidation | breakout | cheers | moom | proud | holdings | m | comparing | big | boys | single | page | sell | ll | january | pst | forget | webull | htmw | won | let | trade | fake | money | blackberry | dip | market | level | data | uk | brothers | sisters | freetrade | allowing | gme | bb | amc | trading | instant | account | topups | o | counterfeit | shares | right | public | won | let | buys | dip | unite | machine | ve | won | evil | doers | left | end | trade | forced | yolo | gamestop | like | stock | flowers | dd | palantir | pltr | low | company | important | fail | unpinned | daily | discussion | thread | february | seen | shit | etoro | allowing | buy | gme | knows | slv | jumped | suddenly | imgn | roof | underground | rocket | bb | nmrk | yolo | update | aal | herd | immunity | april | march | calls | buy | stocks | little | late | thought | appreciated | days | denzel | washington | said | don | read | newspaper | uninformed | read | newsp | found | nice | melt | begins | buy | dip | amc | moon | got | lil | closer | yesterday | followed | advice | tlry | hrs | today | let | talk | wkhs | osk | xp | holding | strong | retards | strong | explain | things | idiots | puts | couple | clarifications | calls | portfolio | tanked | today | bought | dip | m | holding | hamilton | meets | gamestop | rallying | cry | r | wallstreetbets | exclusive | proudly | ptgauth | t | matter | rough | gets | nt | sell | hold | hold | prevail | buy | nok | jul | jul | jul | jul | jul | jul | jul | jul | aug | aug | jul | jul | aug | jul | jul | jul | jul | jul | jul | jul | jul | aug | aug | jul | jul | aug\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "doc = nlp(content)\n",
    "# print(doc)\n",
    "\n",
    "# Remove whitespace/punctuation/stopwords and lower case tokens\n",
    "tokens = [\n",
    "    word.lower_\n",
    "    for word in doc\n",
    "    if not (word.is_space\n",
    "            or word.is_punct \n",
    "            or word.is_stop \n",
    "            or word.is_digit \n",
    "            or word.is_currency \n",
    "            or re.search(r'\\d', word.text) # Regex to remove digits\n",
    "            )\n",
    "]\n",
    "# print(tokens)\n",
    "\n",
    "freq = Counter(map(str, tokens))\n",
    "# Top 5 most common tokens put into a list\n",
    "common_tokens = [f\"{x}: {y}\" for x, y in freq.most_common(5)]\n",
    "print(\"Top 5 most common tokens:\")\n",
    "print(*common_tokens, sep=\"\\n\")\n",
    "print(*tokens,sep=' | ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common tokens:\n",
      "jul: 20\n",
      "buy: 6\n",
      "aug: 6\n",
      "hold: 5\n",
      "dip: 4\n",
      "value | aren | cover | hold | fucking | stock | party | dude | shady | go | pre | market | hype | diamond | hand | relax | printer | sing | song | people | army | dogeeeee | rkt | dividend | dividend | stimmy | zom | look | good | basically | moonkey | mertgag | ageddon | uwmc | finally | play | market | meaningless | robinhood | aal | american | airlines | year | long | consolidation | breakout | cheer | moom | proud | holding | m | compare | big | boy | single | page | sell | ll | january | pst | forget | webull | htmw | win | let | trade | fake | money | blackberry | dip | market | level | datum | uk | brother | sister | freetrade | allow | gme | bb | amc | trading | instant | account | topup | o | counterfeit | share | right | public | win | let | buy | dip | unite | machine | ve | win | evil | doer | leave | end | trade | force | yolo | gamestop | like | stock | flower | dd | palantir | pltr | low | company | important | fail | unpinned | daily | discussion | thread | february | see | shit | etoro | allow | buy | gme | know | slv | jump | suddenly | imgn | roof | underground | rocket | bb | nmrk | yolo | update | aal | herd | immunity | april | march | call | buy | stock | little | late | think | appreciate | day | denzel | washington | say | don | read | newspaper | uninformed | read | newsp | find | nice | melt | begin | buy | dip | amc | moon | get | lil | close | yesterday | follow | advice | tlry | hrs | today | let | talk | wkhs | osk | xp | hold | strong | retard | strong | explain | thing | idiot | put | couple | clarification | call | portfolio | tank | today | buy | dip | m | hold | hamilton | meet | gamestop | rallying | cry | r | wallstreetbet | exclusive | proudly | ptgauth | t | matter | rough | get | not | sell | hold | hold | prevail | buy | nok | jul | jul | jul | jul | jul | jul | jul | jul | aug | aug | jul | jul | aug | jul | jul | jul | jul | jul | jul | jul | jul | aug | aug | jul | jul | aug\n"
     ]
    }
   ],
   "source": [
    "# Remove whitespace/punctuation/stopwords and lower case tokens\n",
    "tokens = [\n",
    "    word\n",
    "    for word in doc\n",
    "    if not (\n",
    "        word.is_space\n",
    "        or word.is_punct\n",
    "        or word.is_stop\n",
    "        or word.is_digit\n",
    "        or word.is_currency\n",
    "        or re.search(r\"\\d\", word.text)  # Regex to remove digits\n",
    "    )\n",
    "]\n",
    "\n",
    "# Lemmatize and lowercase\n",
    "lemmas = [word.lemma_.lower() for word in tokens]\n",
    "\n",
    "freq = Counter(map(str, lemmas))\n",
    "# Top 5 most common lemmas put into a list\n",
    "common_lemmas = [f\"{x}: {y}\" for x, y in freq.most_common(5)]\n",
    "print(\"Top 5 most common tokens:\")\n",
    "print(*common_lemmas, sep=\"\\n\")\n",
    "print(*lemmas, sep=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell\n",
    "\n",
    "* Cutoff Score (tokens): \n",
    "* Cutoff Score (lemmas):\n",
    "\n",
    "Feel free to change these scores as you generate your summaries.  Ideally, we're shooting for at least 6 sentences for our summary, but don't want more than 10 (these numbers are rough estimates; they depend on the length of your article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
